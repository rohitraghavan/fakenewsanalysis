{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lab 6 - Word2Vec Tutorial using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data  - Your own data .. Sample from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "# sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar \n",
    "## Wilde from Project Gutenberg : https://www.gutenberg.org\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/844/pg844.txt\" ## Your raw text file location \n",
    "resp = urlopen(url)\n",
    "raw = resp.read().decode('utf8')\n",
    "firstlook = tokenize.sent_tokenize(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = r'''(?x)  # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "|\\w+(?:[-']\\w+)*    # words with optional internal hyphens\n",
    "|\\$?\\d+(?:\\.\\d+)?   # currency, e.g. $12.80 \n",
    "|\\.\\.\\.             # elipses\n",
    "|[.,;\"'?()-_`]      # these are separate tokens\n",
    "'''\n",
    "#print(nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw =\" \".join( nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw=tokenize.sent_tokenize(tokenized_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the text.** Now adjust the output by normalizing the text: things you can try include lowercasing the text, improving the tokenization, and/or doing other adjustments to bring content words higher up in the results. \n",
    "\n",
    "(Note: Readers with NLP background, do it is generally not advised to remove stop words as they can throw context which the algorithm can learn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nopunct=[]\n",
    "for sent in tokenized_raw:\n",
    "    a=[w for w in sent.split() if w not in string.punctuation]\n",
    "    nopunct.append(\" \".join(a))\n",
    "#create a set of stopwords\n",
    "tok_corp= [nltk.word_tokenize(sent) for sent in nopunct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### creating a list of unique words \n",
    "\n",
    "combined_list=[\" \".join(w) for w in tok_corp]\n",
    "unique_list=[]\n",
    "for sent in combined_list:\n",
    "    unique_list.append([w for w in sent.split()])\n",
    "unique_list=list(set([item for sublist in unique_list for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words=unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Loading pretrained word2Vec from Google  ; Include relationships "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For part 1 of the assignment we will use a pretrained word2vec model from Google\n",
    "## Essentially Google had trained the Word2Vec model on a large cropus of data (billion words plus)\n",
    "## You can download this pretrained model from https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "## Once you donload save unzip the file and you should will get another zip file named\n",
    "## GoogleNews-vectors-negative300.bin. Now you are good to go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use the following command to load up a pretrained model \n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4deee3185b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Sample for similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## Sample for similarity \n",
    "\n",
    "print (model.most_similar(positive=['king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainig Gensim using your own dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Its just one single command\n",
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 16, window=5)\n",
    "\n",
    "## Window size : The parameter that fixes the length of window of words by which the model sweeps \n",
    "# the data\n",
    "\n",
    "## Size: Size of the vector\n",
    "\n",
    "## min_count: accept a word if it has a certain minimum number of occurances in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extracting the respective vectors corresponding to the words\n",
    "\n",
    "vector_list=[] ## n by d matrix contaiing words and their respective vectors\n",
    "for word in unique_words:\n",
    "    vector_list.append(model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3464"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension redcution using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3464\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3464\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3464\n",
      "[t-SNE] Computed conditional probabilities for sample 3464 / 3464\n",
      "[t-SNE] Mean sigma: 0.030298\n",
      "[t-SNE] Iteration 25: error = 1.9606242, gradient norm = 276.5291138\n",
      "[t-SNE] Iteration 50: error = 1.9606950, gradient norm = 26927042560.0000000\n",
      "[t-SNE] Iteration 50: error difference 0.000000. Finished.\n",
      "[t-SNE] Iteration 75: error = 1.9606950, gradient norm = 9184924314254704640.0000000\n",
      "[t-SNE] Iteration 75: error difference 0.000000. Finished.\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 1.960695\n",
      "[t-SNE] Iteration 100: error = 1.9606950, gradient norm = 5223895970449850144326156288.0000000\n",
      "[t-SNE] Iteration 100: error difference 0.000000. Finished.\n",
      "[t-SNE] Error after 100 iterations: 1.960695\n"
     ]
    }
   ],
   "source": [
    "## A popular non-linear dimensionality reduction technique that preserves greatly thge local \n",
    "## and global structure of the data. Essentially tries to reconstruct the subspace in which the \n",
    "## data exists\n",
    "## We will use the Sklearn TSNE implementation \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Lets dim reduce the 16 dimension vectors to 2dimensions to vizualise the dataset \n",
    "data_embed=TSNE(n_components=2, perplexity=50, verbose=2, method='barnes_hut').fit_transform(vector_list)\n",
    "\n",
    "\n",
    "## Parameters\n",
    "## n_components = number of dimensions you want your data to be reduced\n",
    "## preplexity =  Number of neighboours to fit the gaussian , normally 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Vizualise\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAETCAYAAADETubIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGDlJREFUeJzt3X1wXXd95/H311iylTgOSVeEEGOpIQRnaU1xJ5Q07K4c\n4jZAeWjDTvAwCwEPDZiHdNhleAjZmIFSoNth6bJOAqgUOo3spgylMNA4XSwybAk2iRPTOE8Q5IQQ\n4ru7IWAwkWx/949zZBQ7jq+sq3t1f3q/ZjS6D+ee85XifPS7v9/3nBuZiSSpuy3odAGSpJkzzCWp\nAIa5JBXAMJekAhjmklQAw1ySCtDWMI+I4Yh4OCJ2NrHtZRGxMyJ2RMRNEbFiynOvj4h7IuLuiHjd\n7FYtSXNftLPPPCJeBOwFPp+ZK4+x7ZLM3FvffjmwPjNfEhGnAN8BVgEB3AKsysxHZ7d6SZq72joy\nz8xvAo9MfSwizoyIr0XE9oj4RkScXW+7d8pmS4CD9e3fB7Zk5qOZ+RNgC3BRG8qXpDlrYacLAD4F\nXJaZ34+IFwBXAy8GiIj1wDuBHuCCevszgAemvP7B+jFJmrc6GuYRcSLwu8D1ERH1wz2Tz2fmRmBj\nRLwGuBK4tO1FSlIX6PTIfAHwSGauOsZ2m4Fr6tsPAkNTnlsGbG19aZLUPTrRmhj1F5n5M+AHEfHq\nQ09GrKy/nzXlNX8A3FPfvgFYExEn14uha+rHJGneauvIPCKuoxpV/1pE3A9cBbwWuCYi3l/XswnY\nCbwtIi4ExqkWTV8PkJmPRMQHqTpaEvhAvRAqSfNWW1sTJUmzwzNAJakAhrkkFaBtc+YR4XyOJB2H\nzIxjbdPuM0C79uuqq67qeA3ztf5urt36O//V7fU3y2kWSSqAYS5JBTDMmzQ0NNTpEmakm+vv5trB\n+jut2+tvVtv6zCMi23UsSSpFRJBzbQFUkjQ7DHNJKoBhLkkFMMwlqQCGuSQVwDCXpAIY5pJUAMNc\nkgpgmEtSAQxzSSqAYS5JBTDM9TiNRoPt27fTaDQ6XYqkaWhJmEfEyRFxfUTcGRF3RMTvtGK/aq+R\nkc0MDKxgzZo3MzCwgpGRzZ0uSVKTWnLVxIj4a+AbmfnZiFgInJCZPz1sG6+aOIc1Gg0GBlawb99W\nYCWwk76+1ezefRf9/f2dLk+at9p21cSIWAr8u8z8LEBm7j88yDX3jY2N0ds7SBXkACvp6RlgbGys\nc0VJalorpll+Hfg/EfHZiLg1Ij4VEX0t2K/aaHBwkPHxMWBn/chOJiZ2Mzg42LmiJDVtYYv2sQp4\na2Z+JyL+O/Ae4KrDN9ywYcOh20NDQ/PmE0C6QX9/P8PDG1m3bjU9PQNMTOxmeHijUyxSm42OjjI6\nOjrt1814zjwiTgO+lZln1vdfBLw7M19+2HbOmXeBRqPB2NgYg4ODBrk0BzQ7Zz7jkXlmPhwRD0TE\n2Zl5D/BiYNdM96vO6O/vN8SlLtSqbpbnAZ8BeoD7gDdk5qOHbePIXJKmqdmRuR/oLElzmB/oLEnz\niGEuSQUwzCWpAIa5JBXAMJekAhjmklQAw1ySCmCYS1IBDHNJKoBhLkkFMMwlqQCGuSQVwDCXpAIY\n5pJUAMNckgpgmEtSAQxzSSqAYS5JBTDMJakAhrkkFcAwl6QCGOaSVADDXJIKYJhLUgEMc0kqgGEu\nSQUwzCWpAIa5JBXAMJekAhjmklQAw1ySCmCYS1IBDHNJKoBhLkkFMMwlqQAtC/OIWBARt0bEP7Zq\nn5Kk5rRyZH45sKuF+5MkNaklYR4Ry4CXAp9pxf4kSdPTqpH5x4F3Admi/UmSpmHGYR4RLwMezszb\ngKi/JElttLAF+zgfeEVEvBToA06KiM9n5usO33DDhg2Hbg8NDTE0NNSCw0tSOUZHRxkdHZ326yKz\ndTMjEfEfgP+cma94gueylceSpPkgIsjMY8542GcuSQVo6cj8SQ/kyFySps2RuSTNI4a5JBXAMJek\nAhjmklQAw1ySCmCYS1IBDHNJKoBhLkkFMMwlqQCGuSQVwDCXpAIY5pJUAMNckgpgmEtSAQxzSSqA\nYS5JBTDMJakAhrkkFcAwl6QCGOaSVADDXJIKYJhLUgEMc0kqgGEuSQUwzCWpAIa5JBXAMJekAhjm\nklQAw1ySCmCYS1IBDHNJKoBhLkkFMMwlqQCGeSEajQbbt2+n0Wh0uhRJHWCYF2BkZDMDAytYs+bN\nDAysYGRkc6dLktRmkZntOVBEtutY80mj0WBgYAX79m0FVgI76etbze7dd9Hf39/p8iTNUESQmXGs\n7WY8Mo+IZRHx9Yi4IyK+GxHvmOk+1byxsTF6ewepghxgJT09A4yNjXWuKElt14pplv3AOzPzucB5\nwFsjYkUL9qsmDA4OMj4+BuysH9nJxMRuBgcHO1eUpLabcZhn5o8z87b69l7gTuCMme5Xzenv72d4\neCN9fatZunQVfX2rGR7e6BSLNM+0dM48IgaBUeA36mCf+pxz5rOo0WgwNjbG4ODgMYN8OttK6qxm\n58wXtvCAS4C/By4/PMgnbdiw4dDtoaEhhoaGWnX4ea+/v7+pYB4Z2cy6devp7a2mZ4aHN7J27SVt\nqFBSM0ZHRxkdHZ3261oyMo+IhcBXgK9l5ieOso0j8w6z80XqPm3rZqn9FbDraEGuucHOF6lcrWhN\nPB94LXBBROyIiFsj4qKZl6ZWs/NFKteM58wz838DT2lBLZplk50v69atpqdngImJ3Xa+SIXwDNB5\nyG4WqXs0O2dumHchw1iaP9q9AKo28aJakp6II/MuYmuhNP84Mi+QrYWSjsYw7yK2Fko6GsO8i3hR\nLUlH45x5F7KbRZo/bE2UpAK4ACpJ84hhLkkFMMwlqQCGuSQVwDCXpAIY5pJUAMNckgpgmEtSAQxz\nSSqAYS5JBTDMJakAhrkkFcAwl6QCGOaSVADDXJIKYJgXrtFosH37dhqNRqdLkTSLDPMZmOtBOTKy\nmeXLz2b16v/E8uVnMzKyudMlSZolftLQcRoZ2cy6devp7a0+ZHl4eCNr117S6bIOaTQaPOMZv87+\n/QuAQeCH9PRM8OCD9/lRc1IX8ZOGZlGj0WDduvXs27eVRx+9hX37trJu3fo5NUL/i7/4OPv3HwTO\nBB4E3sPExEF27NjR4cokzQbD/DiMjY3R2zsIrKwfWUlPzwBjY2OdK2qKRqPBxz++EbgZuA3YCnwU\neFpH65I0ewzzwzQzDz44WE2twM76kZ1MTOxmcHCwDRU+uUajwZVXXsX4eD9T/9jAMuAhnvnMZ875\nuX5J02eYTzEyspmBgRWsWfNmBgZWHHXBsL+/n+HhjfT1rWbp0lX09a1meHhjx+eiJ+u/9tp/oppa\n+dUfG7iXRYuexhe+8MWmfkZJ3cUF0Fqj0WBgYAX79m2lGsnupK9vNbt333XUkG40GoyNjTE4ONjx\nIH98/acDbwG+CpwFPAC8nd7evyRzPxMTXwGGaOZnlNRZLoBO0/HMg/f393PuuefOiSDcsWMHCxb0\nA98GnkU1V57ALmAh8GHGx/cyMdEPvBL4MHD6ET/jTKdgnMKROsMwr83lefCjmQzOa6/9NK961Vp+\n/vNx4B3ANcC3gHdRBfojwGlAL/AwcDnwp8Cz+MUv7jn0Mx5rmqnRaLBlyxa2bNlyRFg3Gg0+9KEP\ns3z52U7hSJ2QmW35qg41t1133abs6zs1ly59fvb1nZrXXbep0yUd1WStJ5zw3IS+hNsT/jThhIRV\nCSclPKV+7tn19/MTliYsSnhGwtUJfXnNNZ/KPXv2ZF/fqfV+MuH27Os7Nffs2XPoeD09J9X7Pyt7\ne08+9PuZrAXOSjglYdMRr9+zZ09u27bt0H1Jzamz89gZ28xGx9wJXATcBdwDvPso28z+T90C3RA6\ne/bsyd7ek+vg3ZbwvIRddVBvrcN4zZSQz/p7X8LT6zDvqbdflosWLc0bbrghTzrpN+v97UnIXLr0\n+Yd+F4sXPzXhqUeE/a5du474IwCnJuw59PrJsD/55FVz/o+kNNc0G+YLZzqyj4gFwCeBFwM/ArZH\nxJcy866Z7rsT+vv758Qc+NE0Gg02bdrE+PipVAudO4DvAS8Eng78IbAe+DrVCUOnA9upzgI9A7gP\neB2wGVgK7CFiGddf/wV+9rPvAW+k+s/4Vn75y+9x++23c/PNN3PgwCLgJKauKSxYsIxt27bR2zvI\nvn1T2yAHgBuZmNjNkiVLDp1gVW2zk3XrVnPhhRfM6d+z1G1mHObAC4B7M3M3QERsolph68own8sm\nLyFw4MDTgR9TBfTTgAngvwFPAd4N/B3QA9xPFawBPFbvZQFwHXAQWAw8hV/+8j4+85m/oTrJqApc\neCHj4yfzpje9nSr0fwqM189V2+zffz99fX089th9j3sc7mbx4rcyPHwNe/fuPSLsJxddDXOpdVqx\nAHoGVe/bpB/Wj6mFpl5CYHz8DqrgTapR9ELgY1QLm6PAvcAIsJ8qtKEK/l7gANV/9gX8qn1xEXA2\njz/J6BnAiVR/FH5aP381sBpYBZzH/v2P8cd//FEOHkx6es4/1HP/wQ9ewf3338PatZd05cKy1I3s\nZukSY2NjHDhwGkcG7iKqzpXNwLOnPH85VRCfUD//Pao/AIuB36AK6iuAG+v9PMDjTzL6EVUXzJlU\nI/t7gXOo3nD9F+AgBw58lUcfvYXx8ZtYuLCX66//CLt338X73/++Q6PuuXqClVSaVkyzPAgsn3J/\nWf3YETZs2HDo9tDQEENDQy04/PwwPj4+ZYQ7OZ3xEFVP+UqgQTWtMvn8/cAzgcNP6z8D+AHVSP1n\nwF9SjeA/QDXqXka1jh1U13SZPNb5wO9w0kkrGB8fY8GC09m3b+jQfnt6BjjllFOeMKTXrr2ECy+8\nYM6cYCXNZaOjo4yOjk77dTM+AzQingLcTbUA+hCwDVibmXcetl3O9Fjz2ec+9zkuvfS9VHPfy6l+\n5VCNtEepQve/Us2dP50qsBcDfVOer+bC4WXA/6Kapnkq1SLp3VQj9HuBV1EtrE4eA+As3vOeS/ij\nP3oVS5Ys4bd/+0XTOltW0vFp9gzQVrYm3k2VBO85yjaz2LxTvl27dtWthVvr9sErE3rrrxMSnlX3\neL+o3u43ExYe9nxfQtT3lyR8dMo+9yT8bd2f/sUj2hqn9oxndldPvtTNaLI10WuzdJG3v/1yPvnJ\nT1NNhfyQoaHzuOmmf+HgwclFTag6Wr7Fr0bi51J1oUA1h36AasR+JlWb4r76/ulUb6yWUk2/jAM9\nLFy4nJ6exhN++MZcujaNVKpmR+aGeZe588472bZtG2eddRZr1rxiyoW1/hp4P1VIT53hWkE15XI6\nsIeqB/3dVAuf67j44pfznOc8G4A///NPMDEB1VTNxcAdLFr0SnbsuJlzzjmnLT+fpMdrNsxbsQCq\nNjrnnHM455xz2L59+2H920NUo+rdPH6RtAH8GfA+qjnxjVR95j8hIrn66v95aFTd13ciV175WeDN\nh/a5aNGz2Lt3b5t+OknHy9bELnVk//bPqaZHTgbOo7r07RDwEeBDVOvS91G1J/4CuJHFi5/9uCsm\nXnbZm+jr+3/YEy51H8O8Sx3Zv30xa9f+R+AnVHPjL+FXLYen8vj2xDOB3ezb932WLFnyJPu0J1zq\nFs6Zd7nDFyGvvfbTrF9/OQcPBtWp/g9SzaZNPVX/PKCXxYufyk03/R3nnnvuk+5TUue4ADqPNRoN\nvvSlL/GWt/wJ+/d/heoa5m+hGqH/CPgT4Pfo67vY3nBpjvOThuax/v5+nve853Hiic+hmje/hOo0\ngF+yYEGydOk/0dd3sVMoUkHsZinU4xdIVwIP0de3j1tuuZW9e/c6hSIVxjAv1ORi5rp1q+npGWBi\nYjfDwxvtF5cK5Zx54WZ7MdPFUml2uQCqWTf5YRm9vdWUzhOd8i9pZgzzeapdI+VGo8HAwAqvnCjN\nMrtZ5qGRkc0MDKxgzZo3MzCwgpGRzbN2rLGxMXp7B5l6MtLkx8FJaj9H5oVo90jZkbnUHo7M55l2\nj5Q99V+aWxyZF6JTI2W7WaTZ5SVw55mj9ZXPdsD29/cb4tIc4Mi8MI6UpbLYmijAcJe6nQugamur\noqTOcmReKFsHpTI4Mp/nPKlHml8M80Id+Rmhfp6n5odGo8H27dtpNBqdLqWtDPNCeVKP5qP5vE7k\nnHnh7GbRfFHqOpEnDQnwpB7NH5PrRPv2HblONB/+H3CaRVIR5vs6kWEuqQjzfZ3IOXNJRSltncjT\n+SWpAJ40VJD52jcrqXmG+Rw3n/tmJTXPaZY5rNS+WUnNc5qlAF5fRVKzDPM5bL73zUpqnmE+h833\nvllJzZvRnHlEfAx4OfAY8H3gDZn506Ns65z5cSqtb1ZS89rSZx4RFwJfz8yDEfERIDPzvUfZ1jCX\npGlqywJoZv5zZh6s794MLJvJ/iRJx6eVc+ZvBL7Wwv1Jkpp0zEvgRsSNwGlTHwISuCIzv1xvcwUw\nkZnXPdm+NmzYcOj20NAQQ0ND069Ykgo2OjrK6OjotF8345OGIuJS4E3ABZn52JNs55y5JE1TWz6c\nIiIuAt4F/PsnC3JJ0uyaaTfLvUAv8H/rh27OzPVH2daRuSRNk5fAlaQCeG0WSZpHDHNJKoBhLkkF\nMMwlqQCGuSQVwDCXpAIY5pJUAMNckgpgmEtSAQxzSSqAYS5JBTDMJakAhrkkFcAwl6QCGOZNOp6P\ncZpLurn+bq4drL/Tur3+ZhnmTer2fxDdXH831w7W32ndXn+zDHNJKoBhLkkFaOvHxrXlQJJUmDn1\nGaCSpNnjNIskFcAwl6QCtDXMI+JjEXFnRNwWEV+IiKXtPP5MRcSrI+JfI+JARKzqdD3NiIiLIuKu\niLgnIt7d6XqmIyKGI+LhiNjZ6VqOR0Qsi4ivR8QdEfHdiHhHp2uajohYFBHfjogddf1Xdbqm6YqI\nBRFxa0T8Y6drma6IGIuI2+vf/7Zjbd/ukfkW4LmZ+VvAvcB723z8mfou8IfANzpdSDMiYgHwSeD3\ngecCayNiRWermpbPUtXerfYD78zM5wLnAW/tpt9/Zj4GrM7M5wO/BbwkIl7Q4bKm63JgV6eLOE4H\ngaHMfH5mHvP33tYwz8x/zsyD9d2bgWXtPP5MZebdmXkvcMyV5TniBcC9mbk7MyeATcArO1xT0zLz\nm8Ajna7jeGXmjzPztvr2XuBO4IzOVjU9mfmL+uYiYCHQNR0TEbEMeCnwmU7XcpyCaWR0J+fM3wh8\nrYPHnw/OAB6Ycv+HdFmYlCIiBqlGt9/ubCXTU09T7AB+DNyYmds7XdM0fBx4F130B+gwCdwYEdsj\n4k3H2nhhq48eETcCp019qC7qisz8cr3NFcBEZl7X6uPPVDP1S9MREUuAvwcur0foXaN+J/38en3r\nHyLi32bmnJ+2iIiXAQ9n5m0RMUT3vJue6vzMfCgi+qlC/c763eoTanmYZ+aaJ3s+Ii6leutzQauP\n3QrHqr/LPAgsn3J/Wf2Y2iQiFlIF+d9k5pc6Xc/xysyfRsRW4CK6Yw76fOAVEfFSoA84KSI+n5mv\n63BdTcvMh+rvjYj4ItW06VHDvN3dLBdRve15Rb240s264S/9duCsiBiIiF7gNUC3reoH3fG7Ppq/\nAnZl5ic6Xch0RcS/iYiT69t9wBrgrs5W1ZzMfF9mLs/MM6n+3X+9m4I8Ik6o39EREScCvwf865O9\npt1z5v8DWEL1luHWiNjY5uPPSES8KiIeAF4IfCUi5vScf2YeAN5G1UV0B7ApM+/sbFXNi4jrgH8B\nzo6I+yPiDZ2uaToi4nzgtcAFdXvZrfWAplucDmyNiNuo5vpvyMyvdrim+eI04Jv1esXNwJczc8uT\nvcDT+SWpAJ4BKkkFMMwlqQCGuSQVwDCXpAIY5pI0A9O5IFxEXBYRO+vuppumXqsnIl5fXxDv7oiY\ndhul3SySNAMR8SJgL/D5zFx5jG2XTJ4FHBEvB9Zn5ksi4hTgO8AqqvMqbgFWZeajzdbhyFySZuCJ\nLggXEWdGxNfq66p8IyLOrredejmHJVRXRoTq6qBbMvPRzPwJ1bkh0zonoeWn80uS+BRwWWZ+v75s\n8NXAiwEiYj3wTqCHX13W5PCL4j3INC+KZ5hLUgvVp9//LnB9RExeiqJn8vnM3AhsjIjXAFcCl7bi\nuIa5JLXWAuCRzDzWp5FtBq6pbz8IDE15bhmwdboHlSTNzKELwmXmz4AfRMSrDz0ZsbL+ftaU1/wB\ncE99+wZgTUScXC+Grqkfa5ojc0magfqCcEPAr0XE/cBVVBdYuyYi3k+Vs5uAncDbIuJCYJxq0fT1\nAJn5SER8kKqjJYEP1Auhzddha6IkdT+nWSSpAIa5JBXAMJekAhjmklQAw1ySCmCYS1IBDHNJKoBh\nLkkF+P9OK1zbB2JMAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a2fbc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis= data_embed[:,0]\n",
    "y_axis=data_embed[:,1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis)\n",
    "plt.show() ## The plots vary each time you run them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
